{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 2) (8392, 1)\n",
      "{'author'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train = pd.read_csv(\"data/train.zip\", index_col=['id'])\n",
    "test = pd.read_csv(\"data/test.zip\", index_col=['id'])\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "print(set(train.columns) - set(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndWJwSJI6MTgkSZ38f4xOsmRCGnB9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f80526ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.author = train.author.replace(['EAP', 'HPL', 'MWS'], ['Edgar', 'Howard', 'Merry'])\n",
    "sns.countplot(data=train, x='author');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry\n",
      "\n",
      "Най-чести думи: \n",
      "the    9039\n",
      "of     6129\n",
      "and    5929\n",
      "to     4771\n",
      "I      4155\n",
      "dtype: int64\n",
      "\n",
      "Най-редки думи: \n",
      "potion            1\n",
      "secession         1\n",
      "sentence,         1\n",
      "slumbers,         1\n",
      "precipitation.    1\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "Howard\n",
      "\n",
      "Най-чести думи: \n",
      "the    10330\n",
      "and     5908\n",
      "of      5792\n",
      "a       3230\n",
      "to      3219\n",
      "dtype: int64\n",
      "\n",
      "Най-редки думи: \n",
      "Protestantism    1\n",
      "groping.         1\n",
      "recovery,        1\n",
      "Little           1\n",
      "dissolution.     1\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "Edgar\n",
      "\n",
      "Най-чести думи: \n",
      "the    13927\n",
      "of      8930\n",
      "and     5222\n",
      "to      4625\n",
      "a       4514\n",
      "dtype: int64\n",
      "\n",
      "Най-редки думи: \n",
      "Leonville,    1\n",
      "heavy;        1\n",
      "guest         1\n",
      "Txld          1\n",
      "\"monstrum     1\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_words = {\n",
    "    'Edgar': train[train.author == 'Edgar']['text'].str.split(expand=True).unstack().value_counts(),\n",
    "    'Howard': train[train.author == 'Howard']['text'].str.split(expand=True).unstack().value_counts(),\n",
    "    'Merry': train[train.author == 'Merry']['text'].str.split(expand=True).unstack().value_counts()\n",
    "}\n",
    "\n",
    "for author_name, all_words_by_author in all_words.items():\n",
    "    print(author_name)\n",
    "    print('\\nНай-чести думи: ')\n",
    "    print(all_words_by_author.head())\n",
    "    print('\\nНай-редки думи: ')\n",
    "    print(all_words_by_author.tail())\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(len(stopwords))\n",
    "print(stopwords)\n",
    "\n",
    "params_count_word = {\"features__ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "                      \"features__analyzer\": ['word'],\n",
    "                      \"features__max_df\":[1.0, 0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "                      \"features__min_df\":[2, 3, 5, 10],\n",
    "                      \"features__lowercase\": [False, True],\n",
    "                      \"features__stop_words\": [None, stopwords]}\n",
    "\n",
    "params_count_char = {\"features__ngram_range\": [(1,4), (1,5), (1,6)],\n",
    "                      \"features__analyzer\": ['char'],\n",
    "                      \"features__max_df\":[1.0, 0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "                      \"features__min_df\":[2, 3, 5, 10],\n",
    "                      \"features__lowercase\": [False, True],\n",
    "                      \"features__stop_words\": [None, stopwords]}\n",
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "def random_search():\n",
    "    params = {\n",
    "        \"clf__alpha\": [0.01, 0.1, 0.5, 1, 2]\n",
    "    }\n",
    "\n",
    "    params.update(params_count_word)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features', TfidfVectorizer()),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=params, \n",
    "                                       scoring='neg_log_loss',\n",
    "                                       n_iter=20, cv=3, n_jobs=4)\n",
    "\n",
    "    random_search.fit(train.text, train.author)\n",
    "    report(random_search.cv_results_)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: -0.467 (std: 0.004)\n",
      "Parameters: {'clf__alpha': 0.1, 'features__stop_words': None, 'features__ngram_range': (1, 3), 'features__max_df': 0.6, 'features__analyzer': 'word', 'features__min_df': 5, 'features__lowercase': False}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: -0.469 (std: 0.001)\n",
      "Parameters: {'clf__alpha': 0.01, 'features__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'features__ngram_range': (1, 3), 'features__max_df': 0.7, 'features__analyzer': 'word', 'features__min_df': 3, 'features__lowercase': False}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: -0.469 (std: 0.001)\n",
      "Parameters: {'clf__alpha': 0.01, 'features__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'features__ngram_range': (1, 3), 'features__max_df': 1.0, 'features__analyzer': 'word', 'features__min_df': 3, 'features__lowercase': False}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: -0.500 (std: 0.002)\n",
      "Parameters: {'clf__alpha': 0.1, 'features__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'features__ngram_range': (1, 1), 'features__max_df': 0.7, 'features__analyzer': 'word', 'features__min_df': 5, 'features__lowercase': False}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: -0.502 (std: 0.003)\n",
      "Parameters: {'clf__alpha': 0.1, 'features__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'features__ngram_range': (1, 3), 'features__max_df': 0.8, 'features__analyzer': 'word', 'features__min_df': 5, 'features__lowercase': True}\n",
      "\n",
      "[-0.62277497 -0.62149725 -0.61790357]\n"
     ]
    }
   ],
   "source": [
    "random_search()\n",
    "pipeline = Pipeline([\n",
    "    ('features', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.steps[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.84129902  0.84829911  0.8456705 ]\n",
      "[-0.39106583 -0.38602577 -0.38741785]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# nltk.download('punkt')\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text)]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', TfidfVectorizer(ngram_range=(1, 2),\n",
    "                                 min_df=2,\n",
    "                                 max_df=0.8,\n",
    "                                 lowercase=False,\n",
    "                                 tokenizer=stemming_tokenizer)),\n",
    "#                                  stop_words=stopwords.words('english') + list(string.punctuation))),\n",
    "    ('clf', MultinomialNB(alpha=0.01))\n",
    "])\n",
    "\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline, train.text, train.author, cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: -0.421 (std: 0.003)\n",
      "Parameters: {'clf__alpha': 0.1, 'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__max_df': 0.7, 'features__analyzer': 'word', 'features__min_df': 2, 'features__lowercase': True}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: -0.438 (std: 0.006)\n",
      "Parameters: {'clf__alpha': 0.01, 'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__max_df': 0.8, 'features__analyzer': 'word', 'features__min_df': 3, 'features__lowercase': True}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: -0.458 (std: 0.001)\n",
      "Parameters: {'clf__alpha': 0.01, 'features__stop_words': None, 'features__ngram_range': (1, 1), 'features__max_df': 0.5, 'features__analyzer': 'word', 'features__min_df': 3, 'features__lowercase': False}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: -0.464 (std: 0.004)\n",
      "Parameters: {'clf__alpha': 0.1, 'features__stop_words': None, 'features__ngram_range': (1, 2), 'features__max_df': 0.6, 'features__analyzer': 'word', 'features__min_df': 5, 'features__lowercase': False}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: -0.508 (std: 0.004)\n",
      "Parameters: {'clf__alpha': 0.01, 'features__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'], 'features__ngram_range': (1, 1), 'features__max_df': 0.5, 'features__analyzer': 'word', 'features__min_df': 5, 'features__lowercase': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def random_search():\n",
    "    params = {\n",
    "        \"clf__alpha\": [0.01, 0.1, 0.5, 1, 2]\n",
    "    }\n",
    "\n",
    "    params.update(params_count_word)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features', TfidfVectorizer()),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=params, \n",
    "                                       scoring='neg_log_loss',\n",
    "                                       n_iter=20, cv=3, n_jobs=4)\n",
    "\n",
    "    random_search.fit(train.text, train.author)\n",
    "    report(random_search.cv_results_)\n",
    "random_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search():\n",
    "    params = {\n",
    "        \"clf__alpha\": [0.01, 0.1, 0.5, 1, 2]\n",
    "    }\n",
    "\n",
    "    params.update(params_count_word)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features', TfidfVectorizer(ngram_range=(1, 2),\n",
    "                                     min_df=2,\n",
    "                                     max_df=0.8,\n",
    "                                     lowercase=False,\n",
    "                                     tokenizer=stemming_tokenizer)),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=params, \n",
    "                                       scoring='neg_log_loss',\n",
    "                                       n_iter=20, cv=3, n_jobs=4)\n",
    "\n",
    "    random_search.fit(train.text, train.author)\n",
    "    report(random_search.cv_results_)\n",
    "random_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
